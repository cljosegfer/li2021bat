{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.swin_transformer_1d as module_arch_swin_transformer\n",
    "import model.beat_aligned_transformer as module_arch_beat_aligned_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer_beat_aligned_data\n",
    "# from evaluater import Evaluater_beat_aligned_data\n",
    "from evaluater import Evaluater_beat_aligned_data_CODE as Evaluater_beat_aligned_data\n",
    "# from model.metric import ChallengeMetric\n",
    "from model.metric import CODEMetric as ChallengeMetric\n",
    "from utils.util import load_model\n",
    "from utils.lr_scheduler import CosineAnnealingWarmUpRestarts, GradualWarmupScheduler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_models = {\n",
    "    \"swin_transformer_1d\": ['swin_transformer'],\n",
    "    \"beat_aligned_transformer\": ['beat_aligned_transformer'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-t', '--only_test'], dest='only_test', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, help='only test (default: False)', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = argparse.ArgumentParser(description='PyTorch Template')\n",
    "args.add_argument('-c', '--config', default=None, type=str,\n",
    "                    help='config file path (default: None)')\n",
    "args.add_argument('-r', '--resume', default=None, type=str,\n",
    "                    help='path to latest checkpoint (default: None)')\n",
    "args.add_argument('-d', '--device', default='0', type=str,\n",
    "                    help='indices of GPUs to enable (default: all)')\n",
    "args.add_argument('-s', '--seed', type=int, default=0)\n",
    "args.add_argument('-t', '--only_test', default=False, type=bool,\n",
    "                    help='only test (default: False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomArgs = collections.namedtuple('CustomArgs', 'flags type target')\n",
    "options = [\n",
    "    CustomArgs(['--lr', '--learning_rate'], type=float, target='optimizer;args;lr'),\n",
    "    CustomArgs(['--bs', '--batch_size'], type=int, target='data_loader;args;batch_size')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser.for_notebook(args, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.device_count():  1\n",
      "CUDA_VISIBLE_DEVICES:  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"torch.cuda.device_count(): \", torch.cuda.device_count())\n",
    "print(\"CUDA_VISIBLE_DEVICES: \", os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beat_aligned_transformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(8, 95, kernel_size=(1, 5), stride=(1, 5))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): BasicLayer(\n",
      "      dim=96, input_resolution=80, depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          dim=96, input_resolution=80, num_heads=8, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=96, window_size=(5, 5), num_heads=8\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          dim=96, input_resolution=80, num_heads=8, window_size=5, shift_size=2, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=96, window_size=(5, 5), num_heads=8\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(96, 192, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): ChanNorm()\n",
      "        (2): MaxPool1d(kernel_size=7, stride=2, padding=3, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicLayer(\n",
      "      dim=192, input_resolution=40, depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          dim=192, input_resolution=40, num_heads=16, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=192, window_size=(5, 5), num_heads=16\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          dim=192, input_resolution=40, num_heads=16, window_size=5, shift_size=2, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=192, window_size=(5, 5), num_heads=16\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(192, 384, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): ChanNorm()\n",
      "        (2): MaxPool1d(kernel_size=7, stride=2, padding=3, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicLayer(\n",
      "      dim=384, input_resolution=20, depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          dim=384, input_resolution=20, num_heads=32, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=384, window_size=(5, 5), num_heads=32\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          dim=384, input_resolution=20, num_heads=32, window_size=5, shift_size=2, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=384, window_size=(5, 5), num_heads=32\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(384, 768, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): ChanNorm()\n",
      "        (2): MaxPool1d(kernel_size=7, stride=2, padding=3, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicLayer(\n",
      "      dim=768, input_resolution=10, depth=6\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          dim=768, input_resolution=10, num_heads=64, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=768, window_size=(5, 5), num_heads=64\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          dim=768, input_resolution=10, num_heads=64, window_size=5, shift_size=2, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=768, window_size=(5, 5), num_heads=64\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinTransformerBlock(\n",
      "          dim=768, input_resolution=10, num_heads=64, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=768, window_size=(5, 5), num_heads=64\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinTransformerBlock(\n",
      "          dim=768, input_resolution=10, num_heads=64, window_size=5, shift_size=2, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=768, window_size=(5, 5), num_heads=64\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): SwinTransformerBlock(\n",
      "          dim=768, input_resolution=10, num_heads=64, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=768, window_size=(5, 5), num_heads=64\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): SwinTransformerBlock(\n",
      "          dim=768, input_resolution=10, num_heads=64, window_size=5, shift_size=2, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=768, window_size=(5, 5), num_heads=64\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(768, 1536, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (1): ChanNorm()\n",
      "        (2): MaxPool1d(kernel_size=7, stride=2, padding=3, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (4): BasicLayer(\n",
      "      dim=1536, input_resolution=5, depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock(\n",
      "          dim=1536, input_resolution=5, num_heads=128, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=1536, window_size=(5, 5), num_heads=128\n",
      "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          dim=1536, input_resolution=5, num_heads=128, window_size=5, shift_size=0, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention(\n",
      "            dim=1536, window_size=(5, 5), num_heads=128\n",
      "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "            (act): GELU()\n",
      "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (global_attn): Attention(\n",
      "    (qkv): Linear(in_features=1536, out_features=4608, bias=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "    (act): GELU()\n",
      "    (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (drop_path): Identity()\n",
      "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  (chnorm): ChanNorm()\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (head): Linear(in_features=1536, out_features=6, bias=True)\n",
      "  (head_coarse): Linear(in_features=768, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "logger = config.get_logger('train')\n",
    "\n",
    "# build model architecture, then print to console\n",
    "global model\n",
    "for file, types in files_models.items():\n",
    "    for type in types:\n",
    "        if config[\"arch\"][\"type\"] == type:\n",
    "            model = config.init_obj('arch', eval(\"module_arch_\" + file))\n",
    "            logger.info(model)\n",
    "            if config['arch'].get('weight_path', False):\n",
    "                model = load_model(model, config[\"arch\"][\"weight_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = getattr(module_loss, config['loss']['type'])\n",
    "challenge_metrics = ChallengeMetric(config['data_loader']['args']['label_dir'])\n",
    "metrics = [getattr(challenge_metrics, met) for met in config['metrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = config.init_obj('optimizer', torch.optim, trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"lr_scheduler\"][\"type\"] == \"CosineAnnealingWarmRestarts\":\n",
    "    params = config[\"lr_scheduler\"][\"args\"]\n",
    "    lr_scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=params[\"T_0\"], T_mult=params[\"T_mult\"],\n",
    "                                                    T_up=params[\"T_up\"], gamma=params[\"gamma\"], eta_max=params[\"eta_max\"])\n",
    "elif config[\"lr_scheduler\"][\"type\"] == \"GradualWarmupScheduler\":\n",
    "    params = config[\"lr_scheduler\"][\"args\"]\n",
    "    scheduler_steplr_args = dict(params[\"after_scheduler\"][\"args\"])\n",
    "    scheduler_steplr = getattr(torch.optim.lr_scheduler, params[\"after_scheduler\"][\"type\"])(optimizer, **scheduler_steplr_args)\n",
    "    lr_scheduler = GradualWarmupScheduler(optimizer, multiplier=params[\"multiplier\"], total_epoch=params[\"total_epoch\"], after_scheduler=scheduler_steplr)\n",
    "else:\n",
    "    lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.valid_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer_beat_aligned_data(model, criterion, metrics, optimizer,\n",
    "                          config=config,\n",
    "                          data_loader=data_loader,\n",
    "                          valid_data_loader=valid_data_loader,\n",
    "                          lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, ([data, info], target, class_weights) in enumerate(trainer.data_loader):\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, target, class_weights, info = data.to(device=trainer.device, dtype=torch.float), target.to(trainer.device, dtype=torch.float), \\\n",
    "#                                                 class_weights.to(trainer.device, dtype=torch.float), info.to(trainer.device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = trainer.model(data, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.criterion(output[:, trainer.indices], target[:, trainer.indices]) * class_weights[:, trainer.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/319902 (0%)] Loss: 0.747285, 1 batch cost time 0.70\n",
      "Train Epoch: 1 [512/319902 (0%)] Loss: 0.076057, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [1024/319902 (0%)] Loss: 0.199569, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [1536/319902 (0%)] Loss: 0.102687, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [2048/319902 (1%)] Loss: 0.037557, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [2560/319902 (1%)] Loss: 0.085971, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [3072/319902 (1%)] Loss: 0.096871, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [3584/319902 (1%)] Loss: 0.053256, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [4096/319902 (1%)] Loss: 0.066219, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [4608/319902 (1%)] Loss: 0.119105, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [5120/319902 (2%)] Loss: 0.088440, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [5632/319902 (2%)] Loss: 0.105053, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [6144/319902 (2%)] Loss: 0.131343, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [6656/319902 (2%)] Loss: 0.112705, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [7168/319902 (2%)] Loss: 0.098621, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [7680/319902 (2%)] Loss: 0.128128, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [8192/319902 (3%)] Loss: 0.128785, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [8704/319902 (3%)] Loss: 0.070871, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [9216/319902 (3%)] Loss: 0.097943, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [9728/319902 (3%)] Loss: 0.088378, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [10240/319902 (3%)] Loss: 0.115463, 1 batch cost time 0.51\n",
      "Train Epoch: 1 [10752/319902 (3%)] Loss: 0.115159, 1 batch cost time 0.51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/li2021bat/base/base_trainer.py:76\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# save logged informations into log dict\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     log \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/li2021bat/trainer/trainer.py:95\u001b[0m, in \u001b[0;36mTrainer_beat_aligned_data._train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metrics\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, ([data, info], target, class_weights) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader):\n\u001b[1;32m     96\u001b[0m     batch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     97\u001b[0m     data, target, class_weights, info \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), target\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), \\\n\u001b[1;32m     98\u001b[0m                                         class_weights\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), info\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "File \u001b[0;32m~/miniconda3/envs/li2021bat/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 435\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/li2021bat/lib/python3.8/site-packages/torch/utils/data/dataloader.py:475\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    474\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    477\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/li2021bat/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/li2021bat/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/li2021bat/data_loader/util.py:736\u001b[0m, in \u001b[0;36mCustomTensorDataset_BeatAligned_h5_CODE.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# x = self.tensors[0][0][index]\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# x2 = self.tensors[0][1][index]\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# torch.randn(1)\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecording\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleads_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    737\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratio\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_idx[index], \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/li2021bat/lib/python3.8/site-packages/h5py/_hl/dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer._save_checkpoint(0, save_best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: output/saved/models/beat_aligned_swin_transformer_CODE/0803_005542/model_best.pth ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from epoch 1\n"
     ]
    }
   ],
   "source": [
    "evaluater = Evaluater_beat_aligned_data(model, criterion, metrics,\n",
    "                          config=config, data_loader = data_loader)\n",
    "# evaluater = Evaluater_beat_aligned_data(model, criterion, metrics,\n",
    "#                           config=config, data_loader = data_loader, checkpoint_dir='output/saved/models/beat_aligned_swin_transformer/0622_145002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [04:03,  1.27s/it]\n",
      "185it [01:11,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluater.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/saved/models/beat_aligned_swin_transformer_CODE/0803_004533/metrics.json'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '{}/metrics.json'.format(str(evaluater.checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/saved/results/beat_aligned_swin_transformer_CODE/0803_004533/metrics.json'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pathways = str(evaluater.checkpoint_dir).split('/')\n",
    "# pathways[2] = 'results'\n",
    "# result_dir = '/'.join(pathways)\n",
    "# '{}/metrics.json'.format(str(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_best_thresholds(predictions, true_labels_dict, thresholds):\n",
    "#     num_classes = len(predictions[0])\n",
    "#     best_thresholds = [0.5] * num_classes\n",
    "#     best_f1s = [0.0] * num_classes\n",
    "\n",
    "#     for class_idx in (range(num_classes)):\n",
    "#         for thresh in thresholds:\n",
    "#             f1 = f1_score(\n",
    "#                 true_labels_dict[class_idx],\n",
    "#                 predictions[thresh][class_idx],\n",
    "#                 zero_division=0,\n",
    "#             )\n",
    "\n",
    "#             if f1 > best_f1s[class_idx]:\n",
    "#                 best_f1s[class_idx] = f1\n",
    "#                 best_thresholds[class_idx] = thresh\n",
    "    \n",
    "#     return best_f1s, best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metrics_table(all_binary_results, all_true_labels):\n",
    "#     accuracy_scores = []\n",
    "#     precision_scores = []\n",
    "#     recall_scores = []\n",
    "#     f1_scores = []\n",
    "\n",
    "#     num_classes = all_binary_results.shape[-1]\n",
    "#     for class_idx in range(num_classes):\n",
    "#         class_binary_results = all_binary_results[:, class_idx]\n",
    "#         class_true_labels = all_true_labels[:, class_idx]\n",
    "\n",
    "#         accuracy = accuracy_score(class_true_labels, class_binary_results)\n",
    "#         precision = precision_score(class_true_labels, class_binary_results, zero_division=0)\n",
    "#         recall = recall_score(class_true_labels, class_binary_results, zero_division=0)\n",
    "#         f1 = f1_score(class_true_labels, class_binary_results, zero_division=0)\n",
    "\n",
    "#         accuracy_scores.append(accuracy)\n",
    "#         precision_scores.append(precision)\n",
    "#         recall_scores.append(recall)\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#     metrics_dict = {\n",
    "#         \"Accuracy\": accuracy_scores,\n",
    "#         \"Precision\": precision_scores,\n",
    "#         \"Recall\": recall_scores,\n",
    "#         \"F1 Score\": f1_scores,\n",
    "#     }\n",
    "\n",
    "#     return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 6\n",
    "# thresholds = np.arange(0, 1.01, 0.01)\n",
    "# predictions = {thresh: [[] for _ in range(num_classes)] for thresh in thresholds}\n",
    "# true_labels_dict = [[] for _ in range(num_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluater.model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, ([data, info], target, class_weights) in tqdm(enumerate(evaluater.data_loader.valid_data_loader)):\n",
    "#         data, target, class_weights, info = data.to(device=evaluater.device, dtype=torch.float), target.to(evaluater.device, dtype=torch.float), \\\n",
    "#                                         class_weights.to(evaluater.device, dtype=torch.float), info.to(evaluater.device, dtype=torch.float)\n",
    "#         output = evaluater.model(data, info)\n",
    "\n",
    "#         output_logit = evaluater.sigmoid(output)\n",
    "#         output_logit = evaluater._to_np(output_logit)\n",
    "#         target = evaluater._to_np(target)\n",
    "\n",
    "#         for class_idx in range(num_classes):\n",
    "#             for thresh in thresholds:\n",
    "#                 predicted_binary = (output_logit[:, class_idx] >= thresh)\n",
    "#                 predictions[thresh][class_idx].extend(\n",
    "#                     predicted_binary\n",
    "#                 )\n",
    "#             true_labels_dict[class_idx].extend(\n",
    "#                 target[:, class_idx]\n",
    "#             )\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_f1s, best_thresholds = find_best_thresholds(predictions, true_labels_dict, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_binary_results = []\n",
    "# all_true_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluater.model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, ([data, info], target, class_weights) in tqdm(enumerate(evaluater.data_loader.test_data_loader)):\n",
    "#         data, target, class_weights, info = data.to(device=evaluater.device, dtype=torch.float), target.to(evaluater.device, dtype=torch.float), \\\n",
    "#                                         class_weights.to(evaluater.device, dtype=torch.float), info.to(evaluater.device, dtype=torch.float)\n",
    "#         output = evaluater.model(data, info)\n",
    "\n",
    "#         output_logit = evaluater.sigmoid(output)\n",
    "#         output_logit = evaluater._to_np(output_logit)\n",
    "#         target = evaluater._to_np(target)\n",
    "\n",
    "#         binary_result = np.zeros_like(output_logit)\n",
    "#         for i in range(len(best_thresholds)):\n",
    "#             binary_result[:, i] = (\n",
    "#                 output_logit[:, i] >= best_thresholds[i]\n",
    "#             )\n",
    "        \n",
    "#         all_binary_results.append(binary_result)\n",
    "#         all_true_labels.append(target)\n",
    "#         break\n",
    "\n",
    "# all_binary_results = np.array(all_binary_results)[0, :, :]\n",
    "# all_true_labels = np.array(all_true_labels)[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': [0.0, 0.03125, 0.984375, 0.75, 0.125, 0.125],\n",
       " 'Precision': [0.0,\n",
       "  0.03125,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.017543859649122806,\n",
       "  0.017543859649122806],\n",
       " 'Recall': [0.0, 1.0, 0.0, 0.0, 1.0, 1.0],\n",
       " 'F1 Score': [0.0,\n",
       "  0.06060606060606061,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.034482758620689655,\n",
       "  0.034482758620689655]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics_dict = metrics_table(all_binary_results, all_true_labels)\n",
    "# metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/saved/models/beat_aligned_swin_transformer_CODE/0803_003053/metrics.json'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '{}/metrics.json'.format(str(evaluater.checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{}/metrics.json'.format(str(evaluater.checkpoint_dir)), 'w') as f:\n",
    "#     json.dump(metrics_dict, f, indent = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "li2021bat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
